{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clickHistory</th>\n",
       "      <th>current_news</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N61901 N28165 N5994 N32458 N30740 N63732 N2290...</td>\n",
       "      <td>N57352</td>\n",
       "      <td>0</td>\n",
       "      <td>U7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N61901 N28165 N5994 N32458 N30740 N63732 N2290...</td>\n",
       "      <td>N37113</td>\n",
       "      <td>0</td>\n",
       "      <td>U7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N61901 N28165 N5994 N32458 N30740 N63732 N2290...</td>\n",
       "      <td>N15218</td>\n",
       "      <td>0</td>\n",
       "      <td>U7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N61901 N28165 N5994 N32458 N30740 N63732 N2290...</td>\n",
       "      <td>N63863</td>\n",
       "      <td>0</td>\n",
       "      <td>U7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N61901 N28165 N5994 N32458 N30740 N63732 N2290...</td>\n",
       "      <td>N54656</td>\n",
       "      <td>0</td>\n",
       "      <td>U7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156964</th>\n",
       "      <td>N33347 N58459 N45956 N35685 N15456 N47193 N388...</td>\n",
       "      <td>N40349</td>\n",
       "      <td>0</td>\n",
       "      <td>U63859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156964</th>\n",
       "      <td>N33347 N58459 N45956 N35685 N15456 N47193 N388...</td>\n",
       "      <td>N16697</td>\n",
       "      <td>0</td>\n",
       "      <td>U63859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156964</th>\n",
       "      <td>N33347 N58459 N45956 N35685 N15456 N47193 N388...</td>\n",
       "      <td>N53867</td>\n",
       "      <td>0</td>\n",
       "      <td>U63859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156964</th>\n",
       "      <td>N33347 N58459 N45956 N35685 N15456 N47193 N388...</td>\n",
       "      <td>N61497</td>\n",
       "      <td>0</td>\n",
       "      <td>U63859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156964</th>\n",
       "      <td>N33347 N58459 N45956 N35685 N15456 N47193 N388...</td>\n",
       "      <td>N14382</td>\n",
       "      <td>0</td>\n",
       "      <td>U63859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5723002 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clickHistory current_news label  \\\n",
       "0       N61901 N28165 N5994 N32458 N30740 N63732 N2290...       N57352     0   \n",
       "0       N61901 N28165 N5994 N32458 N30740 N63732 N2290...       N37113     0   \n",
       "0       N61901 N28165 N5994 N32458 N30740 N63732 N2290...       N15218     0   \n",
       "0       N61901 N28165 N5994 N32458 N30740 N63732 N2290...       N63863     0   \n",
       "0       N61901 N28165 N5994 N32458 N30740 N63732 N2290...       N54656     0   \n",
       "...                                                   ...          ...   ...   \n",
       "156964  N33347 N58459 N45956 N35685 N15456 N47193 N388...       N40349     0   \n",
       "156964  N33347 N58459 N45956 N35685 N15456 N47193 N388...       N16697     0   \n",
       "156964  N33347 N58459 N45956 N35685 N15456 N47193 N388...       N53867     0   \n",
       "156964  N33347 N58459 N45956 N35685 N15456 N47193 N388...       N61497     0   \n",
       "156964  N33347 N58459 N45956 N35685 N15456 N47193 N388...       N14382     0   \n",
       "\n",
       "            id  \n",
       "0        U7615  \n",
       "0        U7615  \n",
       "0        U7615  \n",
       "0        U7615  \n",
       "0        U7615  \n",
       "...        ...  \n",
       "156964  U63859  \n",
       "156964  U63859  \n",
       "156964  U63859  \n",
       "156964  U63859  \n",
       "156964  U63859  \n",
       "\n",
       "[5723002 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.读取行为数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: Columnar iteration over characters will be deprecated in future releases.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5723002\n",
      "time is :  44.732322454452515\n"
     ]
    }
   ],
   "source": [
    "print('start')\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "train_behavior = pd.read_csv('./data/train/behaviors.tsv',sep ='\\t',header=None)\n",
    "###处理 behaviors.tsv 行为数据\n",
    "train_behavior.columns = ['index','id','timeStamp','clickHistory','impressionLog']\n",
    "impressionLog = train_behavior['impressionLog'].str.split(' ', expand=True).stack().reset_index(level = 1,drop = True)\n",
    "key, label = impressionLog.str.split('-', 1).str\n",
    "data_new = pd.DataFrame()\n",
    "data_new= train_behavior[['clickHistory']].join(pd.DataFrame(key,columns=['current_news']))\n",
    "data_new['label'] = label\n",
    "data_new['id'] = train_behavior['id']\n",
    "data_new.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "\n",
    "print(data_new.shape[0])\n",
    "print('time is : ',time.time() - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.读取新闻数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "title_len = 10\n",
    "title = pd.read_csv('./data/train/news.tsv',sep='\\t',header=None, encoding='utf-8')\n",
    "title.columns = ['id','category','sub_category','title','abstract','url','title_entity','abstract_entity']\n",
    "title = title[['id','title']]\n",
    "arr = []\n",
    "for index ,row in title.iterrows():\n",
    "    text = row['title'].split(' ')\n",
    "    if(len(text) < title_len):\n",
    "        text.extend(['0'] * (title_len - len(text)))\n",
    "    else:\n",
    "        text = text[:title_len]\n",
    "    text.append('[SEP]')\n",
    "    arr.append(text)\n",
    "title['title'] = arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.读取邻居数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor = pd.read_csv('./data/nearst_4.csv')\n",
    "neighbor['nearst'] = neighbor['nearst'].apply(lambda x : eval(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.多线程生成具体行为数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 0\n",
      "2 : 0\n",
      "3 : 0\n",
      "4 : 0\n",
      "5 : 0\n",
      "6 : 0\n",
      "7 : 0\n",
      "8 : 0\n",
      "9 : 0\n",
      "10 : 0\n",
      "11 : 0\n",
      "12 : 0\n",
      "13 : 0\n",
      "14 : 0\n",
      "15 : 0\n",
      "16 : 0\n",
      "17 : 0\n",
      "18 : 0\n",
      "19 : 0\n",
      "20 : 0\n",
      "21 : 0\n",
      "22 : 0\n",
      "23 : 0\n",
      "24 : 0\n",
      "25 : 0\n",
      "26 : 0\n",
      "27 : 0\n",
      "28 : 0\n",
      "29 : 0\n",
      "30 : 0\n",
      "31 : 0\n",
      "32 : 0\n",
      "33 : 0\n",
      "34 : 0\n",
      "35 : 0\n",
      "36 : 0\n",
      "37 : 0\n",
      "38 : 0\n",
      "39 : 0\n",
      "40 : 0\n",
      "41 : 0\n",
      "42 : 0\n",
      "43 : 0\n",
      "44 : 0\n",
      "45 : 0\n",
      "46 : 0\n",
      "7 : 1000\n",
      "28 : 1000\n",
      "36 : 1000\n",
      "8 : 1000\n",
      "30 : 1000\n",
      "6 : 1000\n",
      "21 : 1000\n",
      "25 : 1000\n",
      "32 : 1000\n",
      "18 : 1000\n",
      "27 : 1000\n",
      "37 : 1000\n",
      "24 : 1000\n",
      "13 : 1000\n",
      "23 : 1000\n",
      "31 : 1000\n",
      "15 : 1000\n",
      "26 : 1000\n",
      "34 : 1000\n",
      "17 : 1000\n",
      "14 : 1000\n",
      "39 : 1000\n",
      "46 : 1000\n",
      "4 : 1000\n",
      "41 : 1000\n",
      "43 : 1000\n",
      "42 : 1000\n",
      "40 : 1000\n",
      "16 : 1000\n",
      "22 : 1000\n",
      "45 : 1000\n",
      "38 : 1000\n",
      "2 : 1000\n",
      "33 : 1000\n",
      "29 : 1000\n",
      "3 : 1000\n",
      "10 : 1000\n",
      "11 : 1000\n",
      "5 : 100019 \n",
      ": 1000\n",
      "35 : 1000\n",
      "20 : 1000\n",
      "12 : 1000\n",
      "9 : 1000\n",
      "1 : 1000\n",
      "44 : 1000\n",
      "13 : 2000\n",
      "8 : 2000\n",
      "41 : 2000\n",
      "21 : 2000\n",
      "7 : 2000\n",
      "34 : 2000\n",
      "28 : 2000\n",
      "45 : 2000\n",
      "27 : 2000\n",
      "32 : 2000\n",
      "36 : 2000\n",
      "39 : 200030\n",
      " : 2000\n",
      "42 : 2000\n",
      "46 : 2000\n",
      "25 : 2000\n",
      "24 : 2000\n",
      "15 : 2000\n",
      "6 : 2000\n",
      "31 : 2000\n",
      "10 : 2000\n",
      "23 : 2000\n",
      "33 : 2000\n",
      "22 : 2000\n",
      "20 : 2000\n",
      "9 : 2000\n",
      "18 : 2000\n",
      "17 : 2000\n",
      "26 : 2000\n",
      "37 : 2000\n",
      "19 : 2000\n",
      "4 : 2000\n",
      "29 :40 : 2000\n",
      " 2000\n",
      "35 : 2000\n",
      "16 : 2000\n",
      "12 : 2000\n",
      "38 : 2000\n",
      "43 : 2000\n",
      "3 : 2000\n",
      "44 : 2000\n",
      "11 : 2000\n",
      "14 : 2000\n",
      "1 : 2000\n",
      "2 : 2000\n",
      "5 : 2000\n",
      "13 : 3000\n",
      "8 : 3000\n",
      "41 : 3000\n",
      "45 : 3000\n",
      "32 : 3000\n",
      "27 : 3000\n",
      "34 : 3000\n",
      "37 : 3000\n",
      "42 : 3000\n",
      "21 : 3000\n",
      "7 : 3000\n",
      "28 : 3000\n",
      "22 : 3000\n",
      "10 : 3000\n",
      "24 : 3000\n",
      "39 : 30001540\n",
      "  : :3000\n",
      " 3000\n",
      "33 : 3000\n",
      "26 : 3000\n",
      "29 : 3000\n",
      "30 : 3000\n",
      "23 : 3000\n",
      "36 : 3000\n",
      "43 : 3000\n",
      "35 : 3000\n",
      "9 : 3000\n",
      "46 : 3000\n",
      "44 : 3000\n",
      "14 : 3000\n",
      "20 : 3000\n",
      "25 : 3000\n",
      "31 : 3000\n",
      "16 : 3000\n",
      "17 : 3000\n",
      "1 : 3000\n",
      "19 : 3000\n",
      "12 : 3000\n",
      "18 : 3000\n",
      "6 : 3000\n",
      "11 : 3000\n",
      "4 : 3000\n",
      "3 : 3000\n",
      "38 : 3000\n",
      "5 : 3000\n",
      "2 : 3000\n",
      "13 : 4000\n",
      "41 : 4000\n",
      "32 : 4000\n",
      "8 : 4000\n",
      "34 : 4000\n",
      "37 : 4000\n",
      "27 : 4000\n",
      "45 : 4000\n",
      "33 : 4000\n",
      "28 : 4000\n",
      "22 : 4000\n",
      "42 : 4000\n",
      "40 : 4000\n",
      "21 : 4000\n",
      "35 : 4000\n",
      "36 : 4000\n",
      "23 : 4000\n",
      "29 : 4000\n",
      "31 : 4000\n",
      "10 : 4000\n",
      "39 : 4000\n",
      "46 : 4000\n",
      "30 : 4000\n",
      "24 : 4000\n",
      "7 : 4000\n",
      "43 25 ::  40004000\n",
      "\n",
      "26 : 4000\n",
      "14 : 4000\n",
      "44 : 4000\n",
      "20 : 4000\n",
      "15 : 4000\n",
      "16 : 4000\n",
      "17 : 4000\n",
      "18 : 4000\n",
      "11 : 4000\n",
      "19 : 4000\n",
      "9 : 4000\n",
      "38 : 4000\n",
      "6 : 4000\n",
      "12 : 4000\n",
      "2 : 4000\n",
      "3 : 4000\n",
      "4 : 4000\n",
      "5 : 4000\n",
      "1 : 4000\n",
      "27 : 5000\n",
      "13 : 5000\n",
      "41 : 5000\n",
      "32 : 5000\n",
      "34 : 5000\n",
      "8 : 5000\n",
      "36 : 5000\n",
      "35 : 5000\n",
      "33 : 5000\n",
      "45 : 5000\n",
      "37 : 5000\n",
      "42 : 5000\n",
      "29 : 5000\n",
      "26 : 5000\n",
      "39 : 5000\n",
      "40 : 5000\n",
      "23 : 5000\n",
      "46 : 5000\n",
      "28 : 5000\n",
      "21 : 5000\n",
      "20 : 5000\n",
      "15 : 5000\n",
      "10 : 5000\n",
      "22 : 5000\n",
      "7 : 5000\n",
      "25 : 5000\n",
      "11 : 5000\n",
      "31 : 5000\n",
      "24 : 5000\n",
      "44 : 5000\n",
      "16 : 5000\n",
      "43 : 5000\n",
      "17 : 5000\n",
      "30 : 5000\n",
      "18 : 5000\n",
      "14 : 5000\n",
      "38 : 5000\n",
      "19 : 5000\n",
      "12 : 5000\n",
      "9 : 5000\n",
      "5 : 5000\n",
      "27 : 6000\n",
      "1 : 5000\n",
      "3 : 5000\n",
      "2 : 5000\n",
      "6 : 5000\n",
      "34 : 6000\n",
      "4 : 5000\n",
      "41 : 6000\n",
      "32 : 6000\n",
      "35 : 6000\n",
      "13 : 6000\n",
      "43 : 6000\n",
      "10 : 6000\n",
      "29 : 6000\n",
      "37 : 6000\n",
      "33 : 6000\n",
      "23 : 6000\n",
      "45 : 6000\n",
      "36 : 6000\n",
      "39 : 6000\n",
      "40 : 6000\n",
      "21 : 6000\n",
      "8 : 6000\n",
      "42 : 6000\n",
      "20 : 6000\n",
      "28 : 6000\n",
      "26 : 6000\n",
      "22 : 6000\n",
      "24 : 6000\n",
      "16 : 6000\n",
      "46 : 6000\n",
      "15 : 6000\n",
      "44 : 6000\n",
      "11 : 6000\n",
      "17 : 6000\n",
      "31 : 6000\n",
      "25 : 6000\n",
      "18 : 6000\n",
      "30 : 6000\n",
      "7 : 6000\n",
      "12 : 6000\n",
      "14 : 6000\n",
      "19 : 6000\n",
      "38 : 6000\n",
      "27 : 7000\n",
      "9 : 6000\n",
      "4 : 6000\n",
      "34 : 7000\n",
      "5 : 6000\n",
      "13 : 7000\n",
      "41 : 7000\n",
      "35 : 7000\n",
      "32 : 7000\n",
      "2 : 6000\n",
      "1 : 6000\n",
      "43 : 7000\n",
      "6 : 6000\n",
      "23 : 7000\n",
      "33 : 7000\n",
      "29 : 7000\n",
      "3 : 6000\n",
      "36 : 7000\n",
      "37 : 7000\n",
      "45 : 7000\n",
      "21 : 7000\n",
      "46 : 7000\n",
      "10 : 7000\n",
      "26 : 7000\n",
      "42 : 7000\n",
      "39 : 7000\n",
      "15 : 7000\n",
      "40 : 7000\n",
      "20 : 7000\n",
      "28 : 7000\n",
      "22 : 7000\n",
      "16 : 7000\n",
      "24 : 7000\n",
      "44 : 7000\n",
      "8 : 7000\n",
      "30 : 7000\n",
      "11 : 7000\n",
      "31 : 7000\n",
      "18 : 7000\n",
      "17 : 7000\n",
      "25 : 7000\n",
      "38 : 7000\n",
      "27 : 8000\n",
      "7 : 7000\n",
      "14 : 7000\n",
      "19 : 7000\n",
      "12 : 7000\n",
      "34 : 8000\n",
      "41 : 8000\n",
      "35 : 8000\n",
      "43 : 8000\n",
      "32 : 8000\n",
      "5 : 7000\n",
      "9 : 7000\n",
      "23 : 8000\n",
      "21 : 8000\n",
      "13 : 8000\n",
      "4 : 7000\n",
      "37 : 8000\n",
      "33 : 8000\n",
      "36 : 8000\n",
      "26 : 8000\n",
      "1 : 7000\n",
      "20 : 8000\n",
      "46 : 8000\n",
      "39 : 8000\n",
      "45 : 8000\n",
      "29 : 8000\n",
      "6 : 7000\n",
      "16 : 8000\n",
      "42 : 8000\n",
      "28 : 8000\n",
      "40 : 8000\n",
      "44 : 8000\n",
      "17 : 8000\n",
      "2 : 7000\n",
      "15 : 8000\n",
      "30 : 8000\n",
      "31 : 8000\n",
      "22 : 8000\n",
      "10 : 8000\n",
      "3 : 7000\n",
      "38 : 8000\n",
      "11 : 8000\n",
      "24 : 8000\n",
      "8 : 8000\n",
      "18 : 8000\n",
      "25 : 8000\n",
      "14 : 8000\n",
      "27 : 9000\n",
      "34 : 9000\n",
      "32 : 9000\n",
      "19 : 8000\n",
      "7 : 8000\n",
      "21 : 9000\n",
      "41 : 9000\n",
      "12 : 8000\n",
      "13 : 9000\n",
      "35 : 9000\n",
      "43 : 9000\n",
      "9 : 8000\n",
      "36 : 9000\n",
      "29 : 9000\n",
      "37 : 9000\n",
      "20 : 9000\n",
      "42 : 9000\n",
      "23 : 9000\n",
      "39 : 9000\n",
      "33 : 9000\n",
      "4526  :: 9000 \n",
      "9000\n",
      "4 : 8000\n",
      "5 : 8000\n",
      "46 : 9000\n",
      "40 : 9000\n",
      "6 : 8000\n",
      "28 : 9000\n",
      "44 : 9000\n",
      "1 : 8000\n",
      "31 : 9000\n",
      "10 :16  :9000 9000\n",
      "\n",
      "17 : 9000\n",
      "15 : 9000\n",
      "22 : 9000\n",
      "38 : 9000\n",
      "30 : 9000\n",
      "24 : 9000\n",
      "11 : 9000\n",
      "14 : 9000\n",
      "25 : 9000\n",
      "3 : 8000\n",
      "2 : 8000\n",
      "32 : 10000\n",
      "27 : 10000\n",
      "18 : 9000\n",
      "8 : 9000\n",
      "34 : 10000\n",
      "23 : 10000\n",
      "19 : 9000\n",
      "41 : 10000\n",
      "21 : 10000\n",
      "35 : 10000\n",
      "36 : 10000\n",
      "43 : 10000\n",
      "20 : 10000\n",
      "42 : 100007\n",
      " : 9000\n",
      "12 : 9000\n",
      "13 : 10000\n",
      "29 : 10000\n",
      "45 : 10000\n",
      "39 : 10000\n",
      "37 : 10000\n",
      "26 : 10000\n",
      "40 : 10000\n",
      "33 : 10000\n",
      "46 : 10000\n",
      "9 : 9000\n",
      "31 : 10000\n",
      "44 : 10000\n",
      "11 : 10000\n",
      "5 : 9000\n",
      "22 : 10000\n",
      "28 : 10000\n",
      "24 : 10000\n",
      "16 : 10000\n",
      "4 : 9000\n",
      "25 : 10000\n",
      "17 : 10000\n",
      "10 : 10000\n",
      "6 : 9000\n",
      "38 : 10000\n",
      "30 : 10000\n",
      "27 : 11000\n",
      "15 : 10000\n",
      "1 : 9000\n",
      "32 : 11000\n",
      "14 : 10000\n",
      "21 : 11000\n",
      "18 : 10000\n",
      "20 : 11000\n",
      "37 : 11000\n",
      "36 : 11000\n",
      "8 : 10000\n",
      "2 : 9000\n",
      "19 : 10000\n",
      "13 : 11000\n",
      "34 : 11000\n",
      "41 : 11000\n",
      "23 : 11000\n",
      "35 : 11000\n",
      "3 : 9000\n",
      "45 : 11000\n",
      "43 : 11000\n",
      "29 : 11000\n",
      "42 : 11000\n",
      "7 : 10000\n",
      "40 : 11000\n",
      "26 : 11000\n",
      "12 : 10000\n",
      "39 : 11000\n",
      "46 : 11000\n",
      "31 : 11000\n",
      "22 : 11000\n",
      "33 : 11000\n",
      "44 : 11000\n",
      "11 : 11000\n",
      "24 : 11000\n",
      "30 : 11000\n",
      "10 : 11000\n",
      "16 : 11000\n",
      "9 : 10000\n",
      "28 : 11000\n",
      "17 : 11000\n",
      "27 : 12000\n",
      "38 : 11000\n",
      "25 : 11000\n",
      "4 : 10000\n",
      "6 : 10000\n",
      "5 : 10000\n",
      "37 : 12000\n",
      "15 : 11000\n",
      "18 : 11000\n",
      "14 : 11000\n",
      "35 : 12000\n",
      "20 : 12000\n",
      "32 : 12000\n",
      "41 : 12000\n",
      "36 : 12000\n",
      "8 : 11000\n",
      "29 : 12000\n",
      "34 : 12000\n",
      "21 : 12000\n",
      "1 : 10000\n",
      "13 : 12000\n",
      "19 : 11000\n",
      "45 : 12000\n",
      "26 : 12000\n",
      "39 : 12000\n",
      "40 : 12000\n",
      "42 : 12000\n",
      "43 : 12000\n",
      "23 : 12000\n",
      "3 : 10000\n",
      "46 : 12000\n",
      "2 : 10000\n",
      "31 : 12000\n",
      "22 : 12000\n",
      "12 : 11000\n",
      "33 : 12000\n",
      "44 : 12000\n",
      "24 : 12000\n",
      "11 : 12000\n",
      "7 : 11000\n",
      "30 : 12000\n",
      "28 : 12000\n",
      "16 : 12000\n",
      "17 : 12000\n",
      "27 : 13000\n",
      "38 : 12000\n",
      "25 : 12000\n",
      "10 : 12000\n",
      "6 : 11000\n",
      "37 : 13000\n",
      "9 : 11000\n",
      "4 : 11000\n",
      "14 : 12000\n",
      "41 : 13000\n",
      "5 : 11000\n",
      "36 : 13000\n",
      "15 : 12000\n",
      "21 : 13000\n",
      "35 : 13000\n",
      "18 : 12000\n",
      "20 : 13000\n",
      "8 : 12000\n",
      "42 : 13000\n",
      "45 : 13000\n",
      "32 : 13000\n",
      "29 : 13000\n",
      "34 : 13000\n",
      "31 : 13000\n",
      "43 : 13000\n",
      "26 : 13000\n",
      "23 : 13000\n",
      "39 : 13000\n",
      "13 : 13000\n",
      "40 : 13000\n",
      "19 : 12000\n",
      "46 : 13000\n",
      "1 : 11000\n",
      "22 : 13000\n",
      "33 : 13000\n",
      "44 : 13000\n",
      "24 : 13000\n",
      "12 : 12000\n",
      "28 : 13000\n",
      "2 : 11000\n",
      "25 : 13000\n",
      "11 : 13000\n",
      "30 : 13000\n",
      "7 : 12000\n",
      "3 : 11000\n",
      "38 : 13000\n",
      "27 : 14000\n",
      "10 : 13000\n",
      "17 : 13000\n",
      "16 : 13000\n",
      "37 : 14000\n",
      "6 : 12000\n",
      "9 : 12000\n",
      "41 : 14000\n",
      "21 : 14000\n",
      "5 : 12000\n",
      "14 : 13000\n",
      "36 : 14000\n",
      "15 : 13000\n",
      "20 : 14000\n",
      "35 : 14000\n",
      "4 : 12000\n",
      "42 : 14000\n",
      "45 : 14000\n",
      "32 : 14000\n",
      "18 : 13000\n",
      "29 : 14000\n",
      "43 : 14000\n",
      "31 : 14000\n",
      "34 : 14000\n",
      "40 : 14000\n",
      "23 :26 14000 \n",
      ": 14000\n",
      "8 : 13000\n",
      "39 : 14000\n",
      "13 : 14000\n",
      "46 : 14000\n",
      "44 : 14000\n",
      "19 : 13000\n",
      "22 : 14000\n",
      "24 : 14000\n",
      "25 : 14000\n",
      "33 : 14000\n",
      "12 : 13000\n",
      "28 : 14000\n",
      "30 : 14000\n",
      "27 : 15000\n",
      "41 : 15000\n",
      "1 : 12000\n",
      "11 : 14000\n",
      "10 : 14000\n",
      "38 : 14000\n",
      "7 : 13000\n",
      "37 : 15000\n",
      "2 : 12000\n",
      "9 : 13000\n",
      "3 : 12000\n",
      "16 : 14000\n",
      "21 : 15000\n",
      "17 : 14000\n",
      "35 : 15000\n",
      "20 : 15000\n",
      "32 : 15000\n",
      "42 : 15000\n",
      "31 : 15000\n",
      "6 : 13000\n",
      "36 : 15000\n",
      "29 : 15000\n",
      "45 : 15000\n",
      "34 : 15000\n",
      "40 : 15000\n",
      "14 : 14000\n",
      "18 : 14000\n",
      "15 : 14000\n",
      "43 : 15000\n",
      "44 : 15000\n",
      "39 : 15000\n",
      "24 : 15000\n",
      "5 : 13000\n",
      "26 : 15000\n",
      "23 : 15000\n",
      "4 : 13000\n",
      "46 : 15000\n",
      "8 : 14000\n",
      "33 : 15000\n",
      "19 : 14000\n",
      "13 : 15000\n",
      "22 : 15000\n",
      "12 : 14000\n",
      "25 : 15000\n",
      "28 : 15000\n",
      "27 : 16000\n",
      "30 : 15000\n",
      "41 : 16000\n",
      "38 : 15000\n",
      "7 : 14000\n",
      "37 : 16000\n",
      "17 : 15000\n",
      "11 : 15000\n",
      "34 : 16000\n",
      "32 : 16000\n",
      "21 : 16000\n",
      "35 : 16000\n",
      "20 : 16000\n",
      "16 : 15000\n",
      "3 : 13000\n",
      "10 : 15000\n",
      "31 : 16000\n",
      "40 : 16000\n",
      "1 : 13000\n",
      "45 : 16000\n",
      "2 : 13000\n",
      "42 : 16000\n",
      "43 : 16000\n",
      "36 : 16000\n",
      "39 : 16000\n",
      "9 : 14000\n",
      "18 : 15000\n",
      "29 : 16000\n",
      "6 : 14000\n",
      "15 : 15000\n",
      "33 : 16000\n",
      "24 : 16000\n",
      "5 : 14000\n",
      "26 : 16000\n",
      "44 : 16000\n",
      "14 : 15000\n",
      "23 : 16000\n",
      "46 : 16000\n",
      "4 : 14000\n",
      "12 : 15000\n",
      "38 : 16000\n",
      "27 : 17000\n",
      "8 : 15000\n",
      "25 : 16000\n",
      "19 : 15000\n",
      "28 : 16000\n",
      "22 : 16000\n",
      "13 : 16000\n",
      "41 : 17000\n",
      "7 : 15000\n",
      "30 : 16000\n",
      "40 : 17000\n",
      "10 : 16000\n",
      "17 : 16000\n",
      "34 37:  : 17000\n",
      "17000\n",
      "11 : 16000\n",
      "35 : 17000\n",
      "32 : 17000\n",
      "42 : 17000\n",
      "36 : 17000\n",
      "20 : 1700021\n",
      " : 17000\n",
      "16 : 16000\n",
      "15 : 16000\n",
      "31 : 17000\n",
      "43 : 17000\n",
      "39 : 17000\n",
      "14 : 16000\n",
      "45 : 17000\n",
      "18 : 16000\n",
      "3 : 14000\n",
      "24 : 17000\n",
      "33 : 17000\n",
      "29 : 17000\n",
      "2 : 14000\n",
      "44 : 17000\n",
      "9 : 15000\n",
      "4 : 15000\n",
      "23 : 17000\n",
      "1 : 14000\n",
      "26 : 17000\n",
      "46 : 17000\n",
      "27 : 18000\n",
      "6 : 15000\n",
      "19 : 16000\n",
      "28 : 17000\n",
      "38 : 17000\n",
      "5 : 15000\n",
      "22 : 17000\n",
      "34 : 18000\n",
      "25 : 17000\n",
      "40 : 18000\n",
      "30 : 17000\n",
      "8 : 16000\n",
      "12 : 16000\n",
      "41 : 18000\n",
      "37 : 18000\n",
      "17 : 17000\n",
      "13 : 17000\n",
      "35 : 18000\n",
      "10 : 17000\n",
      "42 : 18000\n",
      "31 : 18000\n",
      "7 : 16000\n",
      "32 : 18000\n",
      "14 : 17000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 : 18000\n",
      "11 : 17000\n",
      "16 : 17000\n",
      "45 : 18000\n",
      "20 : 18000\n",
      "43 : 18000\n",
      "21 : 18000\n",
      "18 : 17000\n",
      "39 : 18000\n",
      "15 : 17000\n",
      "29 : 18000\n",
      "24 : 18000\n",
      "2 : 15000\n",
      "44 : 18000\n",
      "3 : 15000\n",
      "33 : 18000\n",
      "22 : 18000\n",
      "4 : 16000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "punctuation = '!,;:?\"\\''\n",
    "def removeFormat(text):\n",
    "    result = re.sub(r'[{}]+'.format(punctuation),'',text).strip().lower()\n",
    "    return result\n",
    "def sample_data(data,name):\n",
    "    x_id = []\n",
    "    x_news = []\n",
    "    x_title = []\n",
    "    x_label = []\n",
    "    maxCount = 15\n",
    "    step = 0\n",
    "    for index,row in (data.iterrows()):\n",
    "        if(step > -1 and step % 1000 == 0):\n",
    "            print(name,':',step)\n",
    "        text = []\n",
    "        \n",
    "        if(type( row['clickHistory']) == float): continue\n",
    "        history = row['clickHistory'].split(' ') \n",
    "        idxs = list(range(len(history)))\n",
    "        np.random.shuffle(idxs)\n",
    "        count = 0\n",
    "        for x in idxs:\n",
    "            if(count == maxCount):break\n",
    "            tit = title[title['id']==history[x]]['title'].values[0]\n",
    "            \n",
    "            if type(tit) == float:continue\n",
    "            text.extend(tit)\n",
    "            \n",
    "            count += 1\n",
    "        \n",
    "        tit = title[title['id'] == row['current_news']]['title'].values[0]\n",
    "        \n",
    "            \n",
    "        if type(tit) == float:continue\n",
    "        x_news.append(text)\n",
    "        x_title.append(tit)\n",
    "        x_label.append(int(row['label']))\n",
    "        x_id.append(row['id'])\n",
    "        step += 1\n",
    "       \n",
    "    all_data = pd.DataFrame()\n",
    "    \n",
    "    all_data['id'] = x_id\n",
    "    all_data['history'] = x_news\n",
    "    all_data['current'] = x_title\n",
    "    all_data['label'] = x_label\n",
    "\n",
    "    all_data['history'] = all_data['history'].apply(lambda x : [removeFormat(y) for y in x])\n",
    "    all_data['current'] = all_data['current'].apply(lambda x : [removeFormat(y) for y in x])\n",
    "    all_data.to_csv('./data/train/seprate_10/train_{0}.csv'.format(name),index=False)\n",
    "    print('curernt over: ',name)\n",
    "import threading\n",
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "pool_thread = []\n",
    "groups = 46\n",
    "pool = multiprocessing.Pool(processes=groups)\n",
    "data_temp = data_new.iloc[:]\n",
    "for index in range(groups):\n",
    "    count_data = data_temp.shape[0] // groups\n",
    "    pool.apply_async(sample_data,(data_temp.iloc[index * count_data:(index+1)*count_data,:],index+1))\n",
    "\n",
    "pool.close()\n",
    "pool.join() \n",
    "\n",
    "print('all over!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.整合所有多线程数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.DataFrame()\n",
    "for index in range(1,47):\n",
    "    print(index)\n",
    "    now = pd.read_csv('./data/test/seprate_10/test_{0}.csv'.format(index))\n",
    "    data = pd.concat([data,now],axis=0)\n",
    "data.to_csv('./data/test/seprate_10/all_10.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.训练数据采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 1 ##邻居数量\n",
    "def sample_neg(data,number):\n",
    "    k = 1\n",
    "    shape = data.shape[0]\n",
    "    index = 0\n",
    "    index_arr = []\n",
    "    index_neighbor = []\n",
    "    while index < shape:\n",
    "        flag = True\n",
    "        for his in range(k):\n",
    "            if data.iloc[index+his]['label'] != 0:\n",
    "                flag = False\n",
    "                break\n",
    "        if(flag):\n",
    "            for pos in range(k):\n",
    "                index_neighbor.append(neighbor[neighbor['id'] == data.iloc[pos+index]['id']]['nearst'][:k])\n",
    "                index_arr.append(pos + index)\n",
    "            index += k\n",
    "            if(index + k >= shape):break\n",
    "            while(data.iloc[index]['label'] != 1):\n",
    "                index += 1\n",
    "                if(index + k >= shape):break\n",
    "            if(index + k >= shape):break\n",
    "            index_arr.append(index)\n",
    "            index_neighbor.append(neighbor[neighbor['id'] == data.iloc[index]['id']]['nearst'][:k])\n",
    "        else:\n",
    "            index += 1\n",
    "        if(len(index_arr) >= number):break\n",
    "        if(len(index_arr) % 2000 == 0):print(len(index_arr))\n",
    "        if(index + k >= shape):break \n",
    "    return index_arr,index_neighbor\n",
    "def remove_sep(arr):\n",
    "    temp = []\n",
    "    for s in arr:\n",
    "        if s != '[sep]':\n",
    "            temp.append(s)\n",
    "    temp = temp[:165]\n",
    "    return temp\n",
    "def upper_sep(arr):\n",
    "    temp = []\n",
    "    for s in arr:\n",
    "        if s == '[sep]':\n",
    "            temp.append('[SEP]')\n",
    "        else:\n",
    "            temp.append(s)\n",
    "    temp = temp[:165]\n",
    "    return temp\n",
    "data = pd.read_csv('./data/train/seprate_10/all_10.csv')\n",
    "index_arr,index_neighbbor = sample_neg(data,number=100000)\n",
    "co_neigh = []\n",
    "for index in range(len(index_neighbor)):\n",
    "    co_neigh.append(data[data['id'] == index_neighbor[index]]['history'])\n",
    "data = data.iloc[index_arr]\n",
    "\n",
    "data['history'] = data['history'].apply(lambda y :' '.join(upper_sep(eval(y))))\n",
    "data['current'] = data['current'].apply(lambda y :' '.join(remove_sep(eval(y))))\n",
    "data['neighbor'] = co_neigh\n",
    "data['neighbor'] = data['neighbor'].apply(lambda y :' '.join(upper_sep(eval(y))))\n",
    "np.save('./data/bert/15_10/train_8w.npy',data.values.tolist()[:50000])\n",
    "np.save('./data/bert/15_10/dev_2w.npy',data.values.tolist()[50000:])\n",
    "\n",
    "\n",
    "data = pd.read_csv('./data/test/seprate_10/all_10.csv')\n",
    "co_neigh = []\n",
    "for index in range(len(index_neighbor)):\n",
    "    co_neigh.append(data[data['id'] == index_neighbor[index]]['history'])\n",
    "\n",
    "data['history'] = data['history'].apply(lambda y :' '.join(upper_sep(eval(y))))\n",
    "data['current'] = data['current'].apply(lambda y :' '.join(remove_sep(eval(y))))\n",
    "data['neighbor'] = co_neigh\n",
    "data['neighbor'] = data['neighbor'].apply(lambda y :' '.join(upper_sep(eval(y))))\n",
    "np.save('./data/bert/15_10/test.npy',data.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.生成最终的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: UTF-8 -*-\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "train = np.load('./data/bert/15_10/train_8w.npy')\n",
    "dev = np.load('./data/bert/15_10/dev_2w.npy')\n",
    "# test = np.load('./data/bert/15_10_con/test.npy')\n",
    "    \n",
    "his_len = 150\n",
    "neigh_len = 150\n",
    "def pad(text,his_len):\n",
    "    \n",
    "    arr = text.split(' ')\n",
    "    if(len(arr) < his_len):\n",
    "        arr.extend(['[PAD]'] * (his_len - len(arr)))\n",
    "    else:\n",
    "        arr = arr[:his_len]\n",
    "    return ' '.join(arr)\n",
    "train_file =[]\n",
    "for row in tqdm(train):\n",
    "    row[0] = pad(row[0],his_len)\n",
    "    row[3] =  pad(row[3],neigh_len)\n",
    "    train_file.append(['[CLS] '+ row[0]+ row[3] +  ' [SEP] ' + row[1] + ' [SEP]',row[2]])\n",
    "    \n",
    "pd.DataFrame(train_file).to_csv('./data/bert/15_10_con/train_pair_10w.csv',index = False)\n",
    "dev_file = []\n",
    "for row in dev:\n",
    "    row[0] = pad(row[0],his_len)\n",
    "    row[3] =  pad(row[3],neigh_len)\n",
    "    dev_file.append(['[CLS] '+ row[0]+ row[3] +' [SEP] ' + row[1] + ' [SEP]',row[2]])\n",
    "    \n",
    "pd.DataFrame(dev_file).to_csv('./data/bert/15_10_con/dev_pair_16w.csv',index = False)\n",
    "test_file = []\n",
    "for row in test:\n",
    "    row[0] = pad(row[0],his_len)\n",
    "    row[3] =  pad(row[3],neigh_len)\n",
    "    test_file.append(['[CLS] '+ row[0]+ row[3] +' [SEP] ' + row[1] + ' [SEP]',row[2]])\n",
    "    \n",
    "pd.DataFrame(test_file).to_csv('./data/bert/15_10_con/test_pair_16w.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import extract_embeddings\n",
    "\n",
    "train = np.load('./data/bert/15_10_con/train.npy')\n",
    "dev = np.load('./data/bert/15_10_con/dev.npy')\n",
    "test = np.load('./data/bert/15_10_con/test.npy')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "model_path = './data/outputs/bert_15_10_con'\n",
    "\n",
    "from keras_bert import extract_embeddings, POOL_NSP, POOL_MAX\n",
    "train_pair = pd.read_csv('./data/bert/15_10_con/train_pair.csv')\n",
    "train_pair = train_pair['0'].apply(lambda x: eval(x)).values.tolist()\n",
    "# dev_pair = pd.read_csv('./data/bert/15_10_con/dev_pair.csv')\n",
    "# dev_pair = dev_pair['0'].apply(lambda x: eval(x)).values.tolist()\n",
    "# test_pair = pd.read_csv('./data/bert/15_10_con/test_pair.csv')\n",
    "# test_pair = test_pair['0'].apply(lambda x: eval(x)).values.tolist()\n",
    "train_embed = []\n",
    "for index,row in enumerate(train_pair):\n",
    "    embed = extract_embeddings(model_path, row)[0][0]\n",
    "    train_embed.append(embed)\n",
    "    print(index)\n",
    "    \n",
    "# dev_embed = extract_embeddings(model_path, dev_pair)\n",
    "# test_embed = extract_embeddings(model_path, test_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxlen_history = 120\n",
    "maxlen_curr = 30\n",
    "import numpy as np\n",
    "examples = []\n",
    "step = 0\n",
    "for index,row in all_train.iterrows():\n",
    "    temp = []\n",
    "    text = ' '.join(eval(row['history']))\n",
    "    temp.append(text)\n",
    "    text = ' '.join(eval(row['current']))\n",
    "    temp.append(text)\n",
    "    temp.append(row['label'])\n",
    "    examples.append(temp)\n",
    "    step += 1\n",
    "    if(step % 10000 == 0):print(step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print( maxlen_history)\n",
    "print(maxlen_curr)\n",
    "### 训练数据裁剪\n",
    "k = 1\n",
    "train_1 = []\n",
    "index = 0\n",
    "lens = len(examples)\n",
    "while index <(lens):\n",
    "    flag = True\n",
    "    for his in range(k):\n",
    "        if examples[index+his][2] != 0:\n",
    "            flag = False\n",
    "            break\n",
    "    if(flag):\n",
    "        for pos in range(k):\n",
    "            history = examples[pos + index]\n",
    "            history[0] = history[0]\n",
    "            train_1.append(history)\n",
    "        index += k\n",
    "        if(index + k >= lens):break\n",
    "        while(examples[index][2] != 1):\n",
    "            index += 1\n",
    "            if(index + k >= lens):break\n",
    "        if(index + k >= len(examples)):break\n",
    "        history = examples[index]\n",
    "        train_1.append(history)\n",
    "    else:\n",
    "        index += 1\n",
    "    if(len(train_1) % 2000 == 0):print(len(train_1))\n",
    "    if(index + k >= lens):break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = [('all work and no play', 'makes jack a dull boy'),\n",
    "    ('makes jack a dull boy', 'all work and no play')]\n",
    "embeddings = extract_embeddings(model_path, texts)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[1][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "model_name = './data/outputs/bert_15_10_con'  # 指定需下载的预训练模型参数\n",
    "\n",
    "# BERT 在预训练中引入了 [CLS] 和 [SEP] 标记句子的开头和结尾\n",
    "samples = ['[CLS] are you ok？ 0 [SEP]I amgo fine ! 0 [SEP]']  # 准备输入模型的语句\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenized_text = [tokenizer.tokenize(i) for i in samples]\n",
    "\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(i) for i in tokenized_text]\n",
    "input_ids = torch.LongTensor(input_ids)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./data/train/seprate_10/all_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>history</th>\n",
       "      <th>current</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['democrats', 'who', 'voted', 'to', 'impeach',...</td>\n",
       "      <td>['baby', 'trump', 'balloon', 'slashed', 'at', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['comics', '-', 'pluggers', 'by', 'gary', 'bro...</td>\n",
       "      <td>['trump', 'says', 'china', 'trade', 'talks', '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['walmart', 'sparks', 'panic', 'and', 'confusi...</td>\n",
       "      <td>['highlights', 'of', 'the', '2019', 'sema', 's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['majority', 'disapprove', 'of', 'trump', 'sup...</td>\n",
       "      <td>['snow', 'crab', 'sells', 'for', 'record', 'pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['extremely', 'disturbing', 'top', 'dems', 'al...</td>\n",
       "      <td>['kendall', 'jenner', 'wore', 'the', 'tiniest'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999989</th>\n",
       "      <td>['roger', 'stone', 'lied', 'to', 'congress', '...</td>\n",
       "      <td>['feud', 'between', 'trump', 'advisers', 'unde...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999990</th>\n",
       "      <td>['back', 'to', 'blonde', 'britney', 'spears', ...</td>\n",
       "      <td>['hailey', 'bieber', 'sends', 'aunt', 'hilaria...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999991</th>\n",
       "      <td>['black', 'high', 'school', 'security', 'guard...</td>\n",
       "      <td>['2019', 'peoples', 'choice', 'awards', 'fashi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999992</th>\n",
       "      <td>['the', 'woman', 'who', 'flipped', 'off', 'tru...</td>\n",
       "      <td>['my', 'sister', 'and', 'i', 'were', 'adopted'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999993</th>\n",
       "      <td>['far-reaching', 'snowstorm', 'may', 'take', '...</td>\n",
       "      <td>['drake', 'has', 'something', 'to', 'say', 'to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999994 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  history  \\\n",
       "0       ['democrats', 'who', 'voted', 'to', 'impeach',...   \n",
       "1       ['comics', '-', 'pluggers', 'by', 'gary', 'bro...   \n",
       "2       ['walmart', 'sparks', 'panic', 'and', 'confusi...   \n",
       "3       ['majority', 'disapprove', 'of', 'trump', 'sup...   \n",
       "4       ['extremely', 'disturbing', 'top', 'dems', 'al...   \n",
       "...                                                   ...   \n",
       "999989  ['roger', 'stone', 'lied', 'to', 'congress', '...   \n",
       "999990  ['back', 'to', 'blonde', 'britney', 'spears', ...   \n",
       "999991  ['black', 'high', 'school', 'security', 'guard...   \n",
       "999992  ['the', 'woman', 'who', 'flipped', 'off', 'tru...   \n",
       "999993  ['far-reaching', 'snowstorm', 'may', 'take', '...   \n",
       "\n",
       "                                                  current  label  \n",
       "0       ['baby', 'trump', 'balloon', 'slashed', 'at', ...      0  \n",
       "1       ['trump', 'says', 'china', 'trade', 'talks', '...      0  \n",
       "2       ['highlights', 'of', 'the', '2019', 'sema', 's...      0  \n",
       "3       ['snow', 'crab', 'sells', 'for', 'record', 'pr...      0  \n",
       "4       ['kendall', 'jenner', 'wore', 'the', 'tiniest'...      0  \n",
       "...                                                   ...    ...  \n",
       "999989  ['feud', 'between', 'trump', 'advisers', 'unde...      0  \n",
       "999990  ['hailey', 'bieber', 'sends', 'aunt', 'hilaria...      0  \n",
       "999991  ['2019', 'peoples', 'choice', 'awards', 'fashi...      0  \n",
       "999992  ['my', 'sister', 'and', 'i', 'were', 'adopted'...      0  \n",
       "999993  ['drake', 'has', 'something', 'to', 'say', 'to...      0  \n",
       "\n",
       "[999994 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
