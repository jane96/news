{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "\n",
    "class AdditiveAttention(torch.nn.Module):\n",
    "    def __init__(self, in_dim=100, v_size=200):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.v_size = v_size\n",
    "        # self.v = torch.nn.Parameter(torch.rand(self.v_size))\n",
    "        self.proj = nn.Sequential(nn.Linear(self.in_dim, self.v_size), nn.Tanh())\n",
    "        self.proj_v = nn.Linear(self.v_size, 1)\n",
    "\n",
    "    def forward(self, context):\n",
    "        \"\"\"Additive Attention\n",
    "        Args:\n",
    "            context (tensor): [B, seq_len, in_dim]\n",
    "        Returns:\n",
    "            outputs, weights: [B, seq_len, out_dim], [B, seq_len]\n",
    "        \"\"\"\n",
    "        # weights = self.proj(context) @ self.v\n",
    "        weights = self.proj_v(self.proj(context)).squeeze(-1)\n",
    "        weights = torch.softmax(weights, dim=-1) # [B, seq_len]\n",
    "        return torch.bmm(weights.unsqueeze(1), context).squeeze(1), weights # [B, 1, seq_len], [B, seq_len, dim]\n",
    "\n",
    "\n",
    "class MultiheadAttentionContainer(torch.nn.Module):\n",
    "    def __init__(self, nhead, in_proj_container, attention_layer, out_proj):\n",
    "        r\"\"\" A multi-head attention container\n",
    "        Args:\n",
    "            nhead: the number of heads in the multiheadattention model\n",
    "            in_proj_container: A container of multi-head in-projection linear layers (a.k.a nn.Linear).\n",
    "            attention_layer: The attention layer.\n",
    "            out_proj: The multi-head out-projection layer (a.k.a nn.Linear).\n",
    "        Examples::\n",
    "            >>> import torch\n",
    "            >>> embed_dim, num_heads, bsz = 10, 5, 64\n",
    "            >>> in_proj_container = InProjContainer(torch.nn.Linear(embed_dim, embed_dim),\n",
    "                                                    torch.nn.Linear(embed_dim, embed_dim),\n",
    "                                                    torch.nn.Linear(embed_dim, embed_dim))\n",
    "            >>> MHA = MultiheadAttentionContainer(num_heads,\n",
    "                                                  in_proj_container,\n",
    "                                                  ScaledDotProduct(),\n",
    "                                                  torch.nn.Linear(embed_dim, embed_dim))\n",
    "            >>> query = torch.rand((21, bsz, embed_dim))\n",
    "            >>> key = value = torch.rand((16, bsz, embed_dim))\n",
    "            >>> attn_output, attn_weights = MHA(query, key, value)\n",
    "            >>> print(attn_output.shape)\n",
    "            >>> torch.Size([21, 64, 10])\n",
    "        \"\"\"\n",
    "        super(MultiheadAttentionContainer, self).__init__()\n",
    "        self.nhead = nhead\n",
    "        self.in_proj_container = in_proj_container\n",
    "        self.attention_layer = attention_layer\n",
    "        self.out_proj = out_proj\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
    "                attn_mask: Optional[torch.Tensor] = None,\n",
    "                bias_k: Optional[torch.Tensor] = None,\n",
    "                bias_v: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            query, key, value (Tensor): map a query and a set of key-value pairs to an output.\n",
    "                See \"Attention Is All You Need\" for more details.\n",
    "            attn_mask, bias_k and bias_v (Tensor, optional): keyword arguments passed to the attention layer.\n",
    "                See the definitions in the attention.\n",
    "        Shape:\n",
    "            - Inputs:\n",
    "            - query: :math:`(L, N, E)`\n",
    "            - key: :math:`(S, N, E)`\n",
    "            - value: :math:`(S, N, E)`\n",
    "            - attn_mask, bias_k and bias_v: same with the shape of the corresponding args in attention layer.\n",
    "            - Outputs:\n",
    "            - attn_output: :math:`(L, N, E)`\n",
    "            - attn_output_weights: :math:`(N * H, L, S)`\n",
    "            where where L is the target length, S is the sequence length, H is the number of attention heads,\n",
    "                N is the batch size, and E is the embedding dimension.\n",
    "        \"\"\"\n",
    "        tgt_len, src_len, bsz, embed_dim = query.size(\n",
    "            -3), key.size(-3), query.size(-2), query.size(-1)\n",
    "        q, k, v = self.in_proj_container(query, key, value)\n",
    "        assert q.size(-1) % self.nhead == 0, \"query's embed_dim must be divisible by the number of heads\"\n",
    "        head_dim = q.size(-1) // self.nhead\n",
    "        q = q.reshape(tgt_len, bsz * self.nhead, head_dim)\n",
    "\n",
    "        assert k.size(-1) % self.nhead == 0, \"key's embed_dim must be divisible by the number of heads\"\n",
    "        head_dim = k.size(-1) // self.nhead\n",
    "        k = k.reshape(src_len, bsz * self.nhead, head_dim)\n",
    "\n",
    "        assert v.size(-1) % self.nhead == 0, \"value's embed_dim must be divisible by the number of heads\"\n",
    "        head_dim = v.size(-1) // self.nhead\n",
    "        v = v.reshape(src_len, bsz * self.nhead, head_dim)\n",
    "\n",
    "        attn_output, attn_output_weights = self.attention_layer(q, k, v, attn_mask=attn_mask,\n",
    "                                                                bias_k=bias_k, bias_v=bias_v)\n",
    "        attn_output = attn_output.reshape(tgt_len, bsz, embed_dim)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        return attn_output, attn_output_weights\n",
    "\n",
    "\n",
    "class ScaledDotProduct(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.0):\n",
    "        r\"\"\"Processes a projected query and key-value pair to apply\n",
    "        scaled dot product attention.\n",
    "        Args:\n",
    "            dropout (float): probability of dropping an attention weight.\n",
    "        Examples::\n",
    "            >>> SDP = torchtext.models.ScaledDotProduct(0.1)\n",
    "            >>> q = torch.randn(256, 21, 3)\n",
    "            >>> k = v = torch.randn(256, 21, 3)\n",
    "            >>> attn_output, attn_weights = SDP(q, k, v)\n",
    "            >>> print(attn_output.shape, attn_weights.shape)\n",
    "            torch.Size([256, 21, 3]) torch.Size([256, 21, 21])\n",
    "        \"\"\"\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
    "                attn_mask: Optional[torch.Tensor] = None,\n",
    "                bias_k: Optional[torch.Tensor] = None,\n",
    "                bias_v: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Uses a scaled dot product with the projected key-value pair to update\n",
    "        the projected query.\n",
    "        Args:\n",
    "            query (Tensor): Projected query\n",
    "            key (Tensor): Projected key\n",
    "            value (Tensor): Projected value\n",
    "            attn_mask (BoolTensor, optional): 3D mask that prevents attention to certain positions.\n",
    "            bias_k and bias_v: (Tensor, optional): one more key and value sequence to be added at\n",
    "                sequence dim (dim=-3). Those are used for incremental decoding. Users should provide\n",
    "                non-None to both arguments in order to activate them.\n",
    "        Shape:\n",
    "            - query: :math:`(L, N * H, E / H)`\n",
    "            - key: :math:`(S, N * H, E / H)`\n",
    "            - value: :math:`(S, N * H, E / H)`\n",
    "            - attn_mask: :math:`(N * H, L, S)`, positions with ``True`` are not allowed to attend\n",
    "                while ``False`` values will be unchanged.\n",
    "            - bias_k and bias_v:bias: :math:`(1, N * H, E / H)`\n",
    "            - Output: :math:`(L, N * H, E / H)`, :math:`(N * H, L, S)`\n",
    "            where L is the target length, S is the source length, H is the number\n",
    "            of attention heads, N is the batch size, and E is the embedding dimension.\n",
    "        \"\"\"\n",
    "        if bias_k is not None and bias_v is not None:\n",
    "            assert key.size(-1) == bias_k.size(-1) and key.size(-2) == bias_k.size(-2) and bias_k.size(-3) == 1, \\\n",
    "                \"Shape of bias_k is not supported\"\n",
    "            assert value.size(-1) == bias_v.size(-1) and value.size(-2) == bias_v.size(-2) and bias_v.size(-3) == 1, \\\n",
    "                \"Shape of bias_v is not supported\"\n",
    "            key = torch.cat([key, bias_k])\n",
    "            value = torch.cat([value, bias_v])\n",
    "            if attn_mask is not None:\n",
    "                _attn_mask = attn_mask\n",
    "                attn_mask = torch.nn.functional.pad(_attn_mask, (0, 1))\n",
    "\n",
    "        tgt_len, head_dim = query.size(-3), query.size(-1)\n",
    "        assert query.size(-1) == key.size(-1) == value.size(\n",
    "            -1), \"The feature dim of query, key, value must be equal.\"\n",
    "        assert key.size() == value.size(), \"Shape of key, value must match\"\n",
    "        src_len = key.size(-3)\n",
    "        batch_heads = max(query.size(-2), key.size(-2))\n",
    "\n",
    "        # Scale query\n",
    "        query, key, value = query.transpose(\n",
    "            -2, -3), key.transpose(-2, -3), value.transpose(-2, -3)\n",
    "        query = query * (head_dim ** -0.5)\n",
    "        if attn_mask is not None:\n",
    "            if attn_mask.dim() != 3:\n",
    "                raise RuntimeError('attn_mask must be a 3D tensor.')\n",
    "            if (attn_mask.size(-1) != src_len) or (attn_mask.size(-2) != tgt_len) or \\\n",
    "               (attn_mask.size(-3) != 1 and attn_mask.size(-3) != batch_heads):\n",
    "                raise RuntimeError('The size of the attn_mask is not correct.')\n",
    "            if attn_mask.dtype != torch.bool:\n",
    "                raise RuntimeError(\n",
    "                    'Only bool tensor is supported for attn_mask')\n",
    "\n",
    "        # Dot product of q, k\n",
    "        attn_output_weights = torch.matmul(query, key.transpose(-2, -1))\n",
    "        if attn_mask is not None:\n",
    "            attn_output_weights.masked_fill_(attn_mask, -1e8,)\n",
    "        attn_output_weights = torch.nn.functional.softmax(\n",
    "            attn_output_weights, dim=-1)\n",
    "        attn_output_weights = torch.nn.functional.dropout(\n",
    "            attn_output_weights, p=self.dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_output_weights, value)\n",
    "        return attn_output.transpose(-2, -3), attn_output_weights\n",
    "\n",
    "\n",
    "class InProjContainer(torch.nn.Module):\n",
    "    def __init__(self, query_proj, key_proj, value_proj):\n",
    "        r\"\"\"A in-proj container to process inputs.\n",
    "        Args:\n",
    "            query_proj: a proj layer for query.\n",
    "            key_proj: a proj layer for key.\n",
    "            value_proj: a proj layer for value.\n",
    "        \"\"\"\n",
    "\n",
    "        super(InProjContainer, self).__init__()\n",
    "        self.query_proj = query_proj\n",
    "        self.key_proj = key_proj\n",
    "        self.value_proj = value_proj\n",
    "\n",
    "    def forward(self,\n",
    "                query: torch.Tensor,\n",
    "                key: torch.Tensor,\n",
    "                value: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        r\"\"\"Projects the input sequences using in-proj layers.\n",
    "        Args:\n",
    "            query, key, value (Tensors): sequence to be projected\n",
    "        Shape:\n",
    "            - query, key, value: :math:`(S, N, E)`\n",
    "            - Output: :math:`(S, N, E)`\n",
    "            where S is the sequence length, N is the batch size, and E is the embedding dimension.\n",
    "        \"\"\"\n",
    "        return self.query_proj(query), self.key_proj(key), self.value_proj(value)\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(nbatch, sz):\n",
    "    r\"\"\"Generate a square mask for the sequence. The masked positions are filled with True.\n",
    "        Unmasked positions are filled with False.\n",
    "    Args:\n",
    "        nbatch: the number of batch size\n",
    "        sz: the size of square mask\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(\n",
    "        0, 1).repeat(nbatch, 1, 1)\n",
    "    return mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
