{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.import声明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import CrossEntropyLoss,BCEWithLogitsLoss\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers  import BertModel,BertConfig,BertTokenizer\n",
    "\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics import roc_auc_score  \n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.数据采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/bert/15_10_con/train_pair.csv')\n",
    "dev = pd.read_csv('./data/bert/15_10_con/dev_pair.csv')\n",
    "test = pd.read_csv('./data/bert/15_10_con/test_pair.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] d.a. seeks murder charge against former fort worth officer who former nfl lineman justin bannan arrested for attempted murder 0 fisherman faces prison fine for cruel act on rare fish serial stowaway arrested at chicago airport for 2nd time 0 insiders predict nfl week 8 winners 0 0 0 0 gabbard hits back at queen of warmongers clinton 0 0 teen breaks into german prison in bid to win back harry dunns family launches legal action against the uk foreign fact checker trumps shiny new talking point about income growth police arrest motorcyclist who led high-speed chase and went home californias legal weed profits going up in smoke 0 0 the woman who flipped off trump has won an election woman 78 gets 22 years for attempted murder of lawyer [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [SEP] how russia meddles abroad for profit cash trolls and a [SEP]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[1232]['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.数据token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:11<00:00, 3370.38it/s]\n",
      "100%|██████████| 10000/10000 [00:02<00:00, 3404.50it/s]\n",
      "100%|██████████| 49956/49956 [00:14<00:00, 3349.41it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "def get_dummies(l, size=2):\n",
    "    res = list()\n",
    "    for i in l:\n",
    "        tmp = [0] * size\n",
    "        tmp[i] = 1\n",
    "        res.append(tmp)\n",
    "    return res\n",
    "def token(data):\n",
    "    arr = data['0'].values.tolist()\n",
    "    \n",
    "    \n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(i.split(' ')) for i in tqdm(arr)]\n",
    "    input_labels = data['1'].values.tolist()\n",
    "    return [input_ids,input_labels]\n",
    "\n",
    "train_data = token(train)\n",
    "dev_data = token(dev)\n",
    "test_data = token(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 100,\n",
       " 11014,\n",
       " 4028,\n",
       " 3715,\n",
       " 2114,\n",
       " 2280,\n",
       " 3481,\n",
       " 4276,\n",
       " 2961,\n",
       " 2040,\n",
       " 2280,\n",
       " 5088,\n",
       " 24062,\n",
       " 6796,\n",
       " 100,\n",
       " 4727,\n",
       " 2005,\n",
       " 4692,\n",
       " 4028,\n",
       " 1014,\n",
       " 19949,\n",
       " 5344,\n",
       " 3827,\n",
       " 2986,\n",
       " 2005,\n",
       " 10311,\n",
       " 2552,\n",
       " 2006,\n",
       " 4678,\n",
       " 3869,\n",
       " 7642,\n",
       " 100,\n",
       " 4727,\n",
       " 2012,\n",
       " 3190,\n",
       " 3199,\n",
       " 2005,\n",
       " 3416,\n",
       " 2051,\n",
       " 1014,\n",
       " 100,\n",
       " 16014,\n",
       " 5088,\n",
       " 2733,\n",
       " 1022,\n",
       " 4791,\n",
       " 1014,\n",
       " 1014,\n",
       " 1014,\n",
       " 1014,\n",
       " 100,\n",
       " 4978,\n",
       " 2067,\n",
       " 2012,\n",
       " 3035,\n",
       " 1997,\n",
       " 100,\n",
       " 7207,\n",
       " 1014,\n",
       " 1014,\n",
       " 9458,\n",
       " 7807,\n",
       " 2046,\n",
       " 2446,\n",
       " 3827,\n",
       " 1999,\n",
       " 7226,\n",
       " 2000,\n",
       " 2663,\n",
       " 2067,\n",
       " 4302,\n",
       " 100,\n",
       " 2155,\n",
       " 18989,\n",
       " 3423,\n",
       " 2895,\n",
       " 2114,\n",
       " 1996,\n",
       " 2866,\n",
       " 3097,\n",
       " 2755,\n",
       " 100,\n",
       " 100,\n",
       " 12538,\n",
       " 2047,\n",
       " 3331,\n",
       " 2391,\n",
       " 2055,\n",
       " 3318,\n",
       " 3930,\n",
       " 2610,\n",
       " 6545,\n",
       " 100,\n",
       " 2040,\n",
       " 2419,\n",
       " 100,\n",
       " 5252,\n",
       " 1998,\n",
       " 2253,\n",
       " 2188,\n",
       " 100,\n",
       " 3423,\n",
       " 17901,\n",
       " 11372,\n",
       " 2183,\n",
       " 2039,\n",
       " 1999,\n",
       " 5610,\n",
       " 1014,\n",
       " 1014,\n",
       " 1996,\n",
       " 2450,\n",
       " 2040,\n",
       " 9357,\n",
       " 2125,\n",
       " 8398,\n",
       " 2038,\n",
       " 2180,\n",
       " 2019,\n",
       " 2602,\n",
       " 2450,\n",
       " 6275,\n",
       " 4152,\n",
       " 2570,\n",
       " 2086,\n",
       " 2005,\n",
       " 4692,\n",
       " 4028,\n",
       " 1997,\n",
       " 5160,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 102,\n",
       " 2129,\n",
       " 3607,\n",
       " 100,\n",
       " 6917,\n",
       " 2005,\n",
       " 5618,\n",
       " 5356,\n",
       " 27980,\n",
       " 1998,\n",
       " 1037,\n",
       " 102]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][1232]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set = TensorDataset(torch.LongTensor(dev_data[0]),torch.FloatTensor(get_dummies(dev_data[1])))\n",
    "dev_loader = DataLoader(dataset=dev_set,batch_size=32,shuffle=True)\n",
    "\n",
    "test_set = TensorDataset(torch.LongTensor(test_data[0][:10000]),torch.FloatTensor(get_dummies(test_data[1][:10000])))\n",
    "test_loader = DataLoader(dataset=test_set,batch_size=32 ,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0.698594Train Epoch: 1 [0/1250 (0%)]\tLoss:0.698594\tAuc:0.500000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"<ipython-input-7-dc2082d9b9d9>\", line 19, in forward\n    output = self.model(x,attention_mask=attention_mask)\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/transformers/modeling_bert.py\", line 762, in forward\n    output_hidden_states=output_hidden_states,\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/transformers/modeling_bert.py\", line 439, in forward\n    output_attentions,\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/transformers/modeling_bert.py\", line 371, in forward\n    hidden_states, attention_mask, head_mask, output_attentions=output_attentions,\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/transformers/modeling_bert.py\", line 315, in forward\n    hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions,\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/transformers/modeling_bert.py\", line 239, in forward\n    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\nRuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 4.29 GiB already allocated; 17.12 MiB free; 315.46 MiB cached)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dc2082d9b9d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/news/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/parallel/parallel_apply.py\", line 60, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"<ipython-input-7-dc2082d9b9d9>\", line 19, in forward\n    output = self.model(x,attention_mask=attention_mask)\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/transformers/modeling_bert.py\", line 762, in forward\n    output_hidden_states=output_hidden_states,\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/transformers/modeling_bert.py\", line 439, in forward\n    output_attentions,\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/transformers/modeling_bert.py\", line 371, in forward\n    hidden_states, attention_mask, head_mask, output_attentions=output_attentions,\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/transformers/modeling_bert.py\", line 315, in forward\n    hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions,\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"/home/huangzhen/anaconda3/envs/news/lib/python3.6/site-packages/transformers/modeling_bert.py\", line 239, in forward\n    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\nRuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 4.29 GiB already allocated; 17.12 MiB free; 315.46 MiB cached)\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_name = 'bert-base-uncased'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Bert_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bert_model,self).__init__()\n",
    "        self.model = BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "        self.model.to(device)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer1 = nn.Linear(768,2)\n",
    "\n",
    "    def forward(self,x,attention_mask=None,labels=None):\n",
    "        output = self.model(x,attention_mask=attention_mask)\n",
    "        x = output[1] #???\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        return x\n",
    "def predict(logits):\n",
    "    res = torch.argmax(logits, 1)\n",
    "    return res\n",
    "\n",
    "model = Bert_model()\n",
    "model.to(device)\n",
    "model=nn.DataParallel(model,device_ids=[0,1]) # multi-GPU\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "accumulation_steps =1\n",
    "epoch = 3\n",
    "loss_collect = []\n",
    "\n",
    "for i in range(epoch) :\n",
    "    \n",
    "    lossing = 0\n",
    "    aucing = 0\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    x = [x for x in range(len(train_data[0]))]\n",
    "    random.seed(2020 + i)\n",
    "    random.shuffle(x)\n",
    "    train_set = TensorDataset(torch.LongTensor(np.array(train_data[0])[x]),torch.FloatTensor(get_dummies(np.array(train_data[1])[x])))\n",
    "    train_loader = DataLoader(dataset=train_set,batch_size=32,shuffle=True)\n",
    "    for batch_idx,(data,target) in enumerate(train_loader):\n",
    "        data,target = Variable(data).to(device),Variable(target).to(device)\n",
    "       \n",
    "        mask = []\n",
    "        for sample in data:\n",
    "            \n",
    "            mask.append([1 if i != 0 else 0 for i in sample])\n",
    "        \n",
    "        mask =torch.Tensor(mask).to(device)\n",
    "           \n",
    "        output = model(data,attention_mask = mask)\n",
    "        pred = predict(output)\n",
    "\n",
    "        loss = criterion(sigmoid(output), target)\n",
    "\n",
    "        # 梯度积累\n",
    "        loss = loss/accumulation_steps\n",
    "        loss.backward()\n",
    "        loss_collect.append(loss.item())\n",
    "        print(\"\\r%f\" % loss, end='')\n",
    "        \n",
    "        if((batch_idx+1) % accumulation_steps) == 0:\n",
    "            # 每  次更新一下网络中的参数\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad() \n",
    "        lossing += loss.item()   \n",
    "        aucing += roc_auc_score(target.data.cpu().numpy().tolist(),get_dummies(pred.data.cpu().numpy().tolist()))\n",
    "        if ((batch_idx+1) % accumulation_steps) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss:{:.6f}\\tAuc:{:.6f}'.format(\n",
    "                i+1, batch_idx, len(train_loader), 100. *\n",
    "                batch_idx/len(train_loader), lossing/(batch_idx+1) * accumulation_steps,\n",
    "                aucing/(batch_idx+1)\n",
    "            ))\n",
    "    torch.save(model, './data/model/model_{}.pkl'.format(i))\n",
    "\n",
    "    \n",
    "print('训练时间：', time.time()-start)\n",
    "               \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "model_name = 'bert-base-uncased'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Bert_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bert_model,self).__init__()\n",
    "        self.model = BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "        self.model.to(device)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer1 = nn.Linear(768,2)\n",
    "\n",
    "    def forward(self,x,attention_mask=None,labels=None):\n",
    "        output = self.model(x,attention_mask=attention_mask)\n",
    "        x = output[1] #???\n",
    "        \n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        return x\n",
    "def predict(logits):\n",
    "    res = torch.argmax(logits, 1)\n",
    "    return res\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "real = []\n",
    "pre = []\n",
    "for index in range(3):\n",
    "    torch.cuda.empty_cache()\n",
    "    model = torch.load( './data/model/model_{}.pkl'.format(index))   \n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(test_loader)):\n",
    "        data = data.to(device)\n",
    "        target = target.long().to(device)\n",
    "\n",
    "        mask = []\n",
    "        for sample in data:\n",
    "            mask.append([1 if i != 0 else 0 for i in sample])\n",
    "        mask = torch.Tensor(mask).to(device)\n",
    "    \n",
    "        output = model(data, attention_mask=mask)\n",
    "        pred = predict(output).data.cpu().numpy().tolist()\n",
    "        real.extend(target.data.cpu().numpy().tolist())\n",
    "        pre.extend(pred)\n",
    "    \n",
    "    \n",
    "    # 准确率应该达到百分之 90 以上\n",
    "    score = roc_auc_score((real),get_dummies(pre))\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = roc_auc_score((real),get_dummies(pre))\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "768 *32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "hidden_dim = 256\n",
    "embedding_dim = 300\n",
    "vocab_size = 10000\n",
    "\n",
    "class bigru_attention(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(bigru_attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru_layers = 2\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 双向GRU，//操作为了与后面的Attention操作维度匹配，hidden_dim要取偶数！\n",
    "        self.bigru = nn.GRU(embedding_dim, hidden_dim // 2, num_layers=gru_layers, bidirectional=True)\n",
    "        # 由nn.Parameter定义的变量都为requires_grad=True状态\n",
    "        self.weight_W = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.weight_proj = nn.Parameter(torch.Tensor(hidden_dim, 1))\n",
    "        # 二分类\n",
    "        self.fc = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "        nn.init.uniform_(self.weight_W, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.weight_proj, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embedding(sentence) # [seq_len, bs, emb_dim]\n",
    "        gru_out, _ = self.bigru(embeds) # [seq_len, bs, hid_dim]\n",
    "        x = gru_out.permute(1, 0, 2)\n",
    "        # # # Attention过程，与上图中三个公式对应\n",
    "        u = torch.tanh(torch.matmul(x, self.weight_W))\n",
    "        att = torch.matmul(u, self.weight_proj)\n",
    "        att_score = F.softmax(att, dim=1)\n",
    "        scored_x = x * att_score\n",
    "        # # # Attention过程结束\n",
    "        \n",
    "        feat = torch.sum(scored_x, dim=1)\n",
    "        y = self.fc(feat)\n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
