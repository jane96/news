{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.import声明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn import CrossEntropyLoss,BCEWithLogitsLoss\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers  import BertModel,BertConfig,BertTokenizer\n",
    "\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics import roc_auc_score  \n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.数据采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/bert/15_10_con/train_pair.csv')\n",
    "dev = pd.read_csv('./data/bert/15_10_con/dev_pair.csv')\n",
    "test = pd.read_csv('./data/bert/15_10_con/test_pair.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] d.a. seeks murder charge against former fort worth officer who former nfl lineman justin bannan arrested for attempted murder 0 fisherman faces prison fine for cruel act on rare fish serial stowaway arrested at chicago airport for 2nd time 0 insiders predict nfl week 8 winners 0 0 0 0 gabbard hits back at queen of warmongers clinton 0 0 teen breaks into german prison in bid to win back harry dunns family launches legal action against the uk foreign fact checker trumps shiny new talking point about income growth police arrest motorcyclist who led high-speed chase and went home californias legal weed profits going up in smoke 0 0 the woman who flipped off trump has won an election woman 78 gets 22 years for attempted murder of lawyer [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [SEP] how russia meddles abroad for profit cash trolls and a [SEP]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[1232]['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.数据token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:18<00:00, 2189.88it/s]\n",
      "100%|██████████| 10000/10000 [00:03<00:00, 3183.06it/s]\n",
      "100%|██████████| 49956/49956 [00:16<00:00, 2996.96it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "def get_dummies(l, size=2):\n",
    "    res = list()\n",
    "    for i in l:\n",
    "        tmp = [0] * size\n",
    "        tmp[i] = 1\n",
    "        res.append(tmp)\n",
    "    return res\n",
    "def token(data):\n",
    "    arr = data['0'].values.tolist()\n",
    "    \n",
    "    \n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(i.split(' ')) for i in tqdm(arr)]\n",
    "    input_labels = data['1'].values.tolist()\n",
    "    return [input_ids,input_labels]\n",
    "\n",
    "train_data = token(train)\n",
    "dev_data = token(dev)\n",
    "test_data = token(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set = TensorDataset(torch.LongTensor(dev_data[0]),torch.FloatTensor(get_dummies(dev_data[1])))\n",
    "dev_loader = DataLoader(dataset=dev_set,batch_size=32,shuffle=True)\n",
    "\n",
    "test_set = TensorDataset(torch.LongTensor(test_data[0][:10000]),torch.FloatTensor(get_dummies(test_data[1][:10000])))\n",
    "test_loader = DataLoader(dataset=test_set,batch_size=32 ,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [7/1250 (1%)]\tLoss:0.711488\tAuc:0.500000\n",
      "Train Epoch: 1 [15/1250 (1%)]\tLoss:0.704543\tAuc:0.500000\n",
      "Train Epoch: 1 [23/1250 (2%)]\tLoss:0.699141\tAuc:0.500000\n",
      "Train Epoch: 1 [31/1250 (2%)]\tLoss:0.697701\tAuc:0.500000\n",
      "Train Epoch: 1 [39/1250 (3%)]\tLoss:0.697611\tAuc:0.500000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f9a30e0bf655>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f9a30e0bf655>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/news/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_name = 'bert-base-uncased'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Bert_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bert_model,self).__init__()\n",
    "        self.model = BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "        self.model.to(device)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer1 = nn.Linear(768,2)\n",
    "\n",
    "    def forward(self,x,attention_mask=None,labels=None):\n",
    "        output = self.model(x,attention_mask=attention_mask)\n",
    "        x = output[1] #???\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        return x\n",
    "def predict(logits):\n",
    "    res = torch.argmax(logits, 1)\n",
    "    return res\n",
    "\n",
    "model = Bert_model()\n",
    "model.to(device)\n",
    "model=nn.DataParallel(model,device_ids=[0,1]) # multi-GPU\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "sigmoid = nn.Sigmoid()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "accumulation_steps =8\n",
    "epoch = 3\n",
    "loss_collect = []\n",
    "\n",
    "for i in range(epoch) :\n",
    "    \n",
    "    lossing = 0\n",
    "    aucing = 0\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    x = [x for x in range(len(train_data[0]))]\n",
    "    random.seed(2020 + i)\n",
    "    random.shuffle(x)\n",
    "    train_set = TensorDataset(torch.LongTensor(np.array(train_data[0])[x]),torch.FloatTensor(get_dummies(np.array(train_data[1])[x])))\n",
    "    train_loader = DataLoader(dataset=train_set,batch_size=32,shuffle=True)\n",
    "    for batch_idx,(data,target) in enumerate(train_loader):\n",
    "        data,target = Variable(data).to(device),Variable(target).to(device)\n",
    "       \n",
    "        mask = []\n",
    "        for sample in data:\n",
    "            \n",
    "            mask.append([1 if i != 0 else 0 for i in sample])\n",
    "        \n",
    "        mask =torch.Tensor(mask).to(device)\n",
    "           \n",
    "        output = model(data,attention_mask = mask)\n",
    "        pred = predict(output)\n",
    "\n",
    "        loss = criterion(sigmoid(output), target)\n",
    "\n",
    "        # 梯度积累\n",
    "        loss = loss/accumulation_steps\n",
    "        loss.backward()\n",
    "        loss_collect.append(loss.item())\n",
    "#         print(\"\\r%f\" % loss, end='')\n",
    "        \n",
    "        if((batch_idx+1) % accumulation_steps) == 0:\n",
    "            # 每  次更新一下网络中的参数\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad() \n",
    "        lossing += loss.item()   \n",
    "        aucing += roc_auc_score(target.data.cpu().numpy().tolist(),get_dummies(pred.data.cpu().numpy().tolist()))\n",
    "        if ((batch_idx+1) % accumulation_steps) == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss:{:.6f}\\tAuc:{:.6f}'.format(\n",
    "                i+1, batch_idx, len(train_loader), 100. *\n",
    "                batch_idx/len(train_loader), lossing/(batch_idx+1) * accumulation_steps,\n",
    "                aucing/(batch_idx+1)\n",
    "            ))\n",
    "    torch.save(model, './data/model/model_{}.pkl'.format(i))\n",
    "\n",
    "    \n",
    "print('训练时间：', time.time()-start)\n",
    "               \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "model_name = 'bert-base-uncased'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Bert_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bert_model,self).__init__()\n",
    "        self.model = BertModel.from_pretrained(\n",
    "            \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "        self.model.to(device)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer1 = nn.Linear(768,2)\n",
    "\n",
    "    def forward(self,x,attention_mask=None,labels=None):\n",
    "        output = self.model(x,attention_mask=attention_mask)\n",
    "        x = output[1] #???\n",
    "        \n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        return x\n",
    "def predict(logits):\n",
    "    res = torch.argmax(logits, 1)\n",
    "    return res\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "real = []\n",
    "pre = []\n",
    "for index in range(3):\n",
    "    torch.cuda.empty_cache()\n",
    "    model = torch.load( './data/model/model_{}.pkl'.format(index))   \n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(test_loader)):\n",
    "        data = data.to(device)\n",
    "        target = target.long().to(device)\n",
    "\n",
    "        mask = []\n",
    "        for sample in data:\n",
    "            mask.append([1 if i != 0 else 0 for i in sample])\n",
    "        mask = torch.Tensor(mask).to(device)\n",
    "    \n",
    "        output = model(data, attention_mask=mask)\n",
    "        pred = predict(output).data.cpu().numpy().tolist()\n",
    "        real.extend(target.data.cpu().numpy().tolist())\n",
    "        pre.extend(pred)\n",
    "    \n",
    "    \n",
    "    # 准确率应该达到百分之 90 以上\n",
    "    score = roc_auc_score((real),get_dummies(pre))\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = roc_auc_score((real),get_dummies(pre))\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "768 *32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "hidden_dim = 256\n",
    "embedding_dim = 300\n",
    "vocab_size = 10000\n",
    "\n",
    "class bigru_attention(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(bigru_attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru_layers = 2\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 双向GRU，//操作为了与后面的Attention操作维度匹配，hidden_dim要取偶数！\n",
    "        self.bigru = nn.GRU(embedding_dim, hidden_dim // 2, num_layers=gru_layers, bidirectional=True)\n",
    "        # 由nn.Parameter定义的变量都为requires_grad=True状态\n",
    "        self.weight_W = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.weight_proj = nn.Parameter(torch.Tensor(hidden_dim, 1))\n",
    "        # 二分类\n",
    "        self.fc = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "        nn.init.uniform_(self.weight_W, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.weight_proj, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embedding(sentence) # [seq_len, bs, emb_dim]\n",
    "        gru_out, _ = self.bigru(embeds) # [seq_len, bs, hid_dim]\n",
    "        x = gru_out.permute(1, 0, 2)\n",
    "        # # # Attention过程，与上图中三个公式对应\n",
    "        u = torch.tanh(torch.matmul(x, self.weight_W))\n",
    "        att = torch.matmul(u, self.weight_proj)\n",
    "        att_score = F.softmax(att, dim=1)\n",
    "        scored_x = x * att_score\n",
    "        # # # Attention过程结束\n",
    "        \n",
    "        feat = torch.sum(scored_x, dim=1)\n",
    "        y = self.fc(feat)\n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
